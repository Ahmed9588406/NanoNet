# NanoNet
This project introduces a compact Autograd engine capable of backpropagation through a dynamically constructed Directed Acyclic Graph (DAG), employing reverse-mode automatic differentiation. Despite its simplicity, with the core functionality condensed into about 100 lines of code and an additional 50 lines for a minimalist neural networks library with a PyTorch-like API, it packs a powerful punch. The DAG processes only scalar values, which means each neuron is broken down into basic operations like tiny additions and multiplications. Nonetheless, this setup is sufficient to construct deep neural networks for tasks like binary classification, as demonstrated in the accompanying notebook. This tiny yet robust implementation is particularly useful for educational purposes.
